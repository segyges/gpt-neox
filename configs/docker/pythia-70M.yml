{
  "num_layers": 6,
  "hidden_size": 512,
  "num_attention_heads": 8,
  "seq_length": 2048,
  "max_position_embeddings": 2048,
  "pos_emb": "rotary",
  "rotary_pct": 0.25,
  "no_weight_tying": true,
  "gpt_j_residual": true,
  "output_layer_parallelism": "column",

  "attention_config": [[["flash"], 6]],
}
